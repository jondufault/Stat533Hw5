---
title: "Homework 5"
author: "Alena Shakhnovich, Nic Walling, Jonathan Dufault"
date: "September 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(ggplot2)
library(dplyr)
library(MASS)
library(kableExtra)
```

## Problem 1

### Algorithm 6.1

> #### Algorithm 6.1: Best subset selection
>
>1. Let $\mathcal{M}_0$ denote the null model, which contains no predictors. This
model simply predicts the sample mean for each observation.
>2. For k = 1, 2,...p:
> (a) Fit all ${p\choose k}$ models that contain exactly k predictors.
> (b) Pick the best among these ${p \choose k}$ models, and call it $\mathcal{M}_k$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$.
> 3. Select a single best model from among $\mathcal{M}_0,\cdots,\mathcal{M}_p$ using cross-validated prediction error, Cp (AIC), BIC, or adjusted R2.




```{r}


# ALMO_ETIP : A Linear Model Over-Engineered To Impress Professor

ALMO_ETIP <- setRefClass("LinearModel",
                           fields=c("dataset",
                                    "response",
                                    ".trainDataset",
                                    ".testDataset",
                                    ".k",
                                    ".featureNames"
                                    ),
                           methods = list(
                             
                             # Initializer function creates 
                             
                             initialize = function(dataset,response,p=0.1){
                               
                               # Deliniates what data is available to predict our response
                               # and the response
                               .self$dataset = dataset
                               .self$response = response
                               .self$.featureNames = colnames(dataset)[colnames(dataset)!=response]
                               
                               # Hoping this saves some computation
                               .self$.k = ncol(dataset) - 1
                               
                               # The test/train data will be used for cross-validation
                               testIndex = .createTestAndTrainDataset(p)
                               .self$.trainDataset = dataset[-c(testIndex),]
                               .self$.testDataset = dataset[testIndex,]
                              
                               },
                             
                             .createTestAndTrainDataset = function(testPct){
                               # Divide the dataset into test and training data based
                               # on the percentage desired to hold for testing
                               n = nrow(.self$dataset)
                               testSize = testPct*n
                               testDataIndex = sample(1:n,testSize)
                               return(testDataIndex)
                             },
                             
                             .crossValidate = function(model,criteria="AdjR2"){
                               print("ma")
                             }
                             
                           )
                           )

ALMO_ETIP$methods(findBestSubset = function(){
  # Let's first Naiively find the best model at each number of features
  bestModelWithiFeatures = list()
  for (i in 1:.self$.k){
    # Easier and clearer to make the objects I'll iterate through,
    # so the code is easier to read
    kChooseiIndices = combn(1:.self$.k,i)

    
    # Can't really think of any better way to do this than "MAX=0, IF GREATER NEW MAX"
    bestr2 = 0
    bestModel = 999
    
    for (j in 1:ncol(kChooseiIndices)){
   
      candidateiFeatures = .self$.featureNames[kChooseiIndices[,j]]

      # This was an unwieldy formula, so I split it up into builders
      xPartOfFormula = paste(candidateiFeatures,collapse="+")
      regressionFormula = as.formula(paste(.self$response,"~",xPartOfFormula))
      
      
      candidateModel = lm(regressionFormula,data=.self$.trainDataset)
      
      if(summary(candidateModel)$r.squared>bestr2){
        bestModel = candidateModel
        bestr2 = summary(candidateModel)$r.squared
      }
    
    }  
      if(bestr2==0){
        stop("Something has gone horribly wrong.")
      }
      else{
        bestModelWithiFeatures[[i]] = bestModel

      }
    

      
    
    
      
  }
  
  
  # Next lets find the best model among the best models. 
  bestModel = .self$.chooseBestModel(bestModelWithiFeatures,"AdjRSq")
  return(bestModel)
})


ALMO_ETIP$methods(.chooseBestModel=function(modelList,criteria){
  # Chooses best model using fit criteria.
  
  if (criteria == "AdjRSq"){
      bestr2 = 0
  bestModel = NA
  for (modelK in modelList){
    if(summary(modelK)$adj.r.squared>bestr2){
      bestr2=summary(modelK)$adj.r.squared
      bestModel = modelK
    }
  }
  }
  
  return(bestModel)
  
  print("sdgkj")
  
})

```

### 6.2
```{r}
ALMO_ETIP$methods(forwardSelection = function(){
  # Let's first Naiively find the best model at each number of features
  bestModelWithiFeatures = list()
   availableFeatures = c(1:.self$.k)
    currentFeatures = c()
  
    for (i in 1:.self$.k){

    # Can't really think of any better way to do this than "MAX=0, IF GREATER NEW MAX"
    bestr2 = 0
    bestModel = 999
    bestIndex = 0
    
    for (j in 1:length(availableFeatures)){
   
      candidateFeatureIndices = c(currentFeatures,availableFeatures[j])
      candidateiFeatures = .self$.featureNames[candidateFeatureIndices]
      # This was an unwieldy formula, so I split it up into builders
      xPartOfFormula = paste(candidateiFeatures,collapse="+")
      regressionFormula = as.formula(paste(.self$response,"~",xPartOfFormula))
      
      
      candidateModel = lm(regressionFormula,data=.self$.trainDataset)
      
      if(summary(candidateModel)$r.squared>bestr2){
        bestModel = candidateModel
        bestr2 = summary(candidateModel)$r.squared
        bestIndex = availableFeatures[j]
      }
    
    }  
      if(bestr2==0){
        stop("Something has gone horribly wrong.")
      }
      else{
        bestModelWithiFeatures[[i]] = bestModel
        availableFeatures = availableFeatures[availableFeatures != bestIndex]
        currentFeatures = c(currentFeatures,bestIndex)
        

      }
    

      
    
    
      
  }
  
  
  # Next lets find the best model among the best models. 
  bestModel = .self$.chooseBestModel(bestModelWithiFeatures,"AdjRSq")
  return(bestModel)
}
)


```


### 6.3

```{r}

ALMO_ETIP$methods(backwardSelection = function(){
  # Let's first Naiively find the best model at each number of features
  bestModelWithiFeatures = list()
  bestModelWithiFeatures[[.self$.k]] = lm(as.formula(paste(.self$response,"~",paste(.self$.featureNames,sep="+"))),data=.self$.trainDataset)
   currentFeatures = c(1:.self$.k)
    for (i in 1:(.self$.k-1)){

    # Can't really think of any better way to do this than "MAX=0, IF GREATER NEW MAX"
    bestr2 = 0
    bestModel = 999
    bestIndex = 0
    
    for (j in 1:length(currentFeatures)){
   
      candidateFeatureIndices = currentFeatures[-j]
      candidateiFeatures = .self$.featureNames[candidateFeatureIndices]
      # This was an unwieldy formula, so I split it up into builders
      xPartOfFormula = paste(candidateiFeatures,collapse="+")
      regressionFormula = as.formula(paste(.self$response,"~",xPartOfFormula))
      
      
      candidateModel = lm(regressionFormula,data=.self$.trainDataset)
      
      if(summary(candidateModel)$r.squared>bestr2){
        bestModel = candidateModel
        bestr2 = summary(candidateModel)$r.squared
        worstIndex = currentFeatures[j]
      }
    
    }  
      if(bestr2==0){
        stop("Something has gone horribly wrong.")
      }
      else{
        bestModelWithiFeatures[[i]] = bestModel
        currentFeatures = currentFeatures[currentFeatures != worstIndex]
        

      }
    

      
    
    
      
  }
  
  
  # Next lets find the best model among the best models. 
  bestModel = .self$.chooseBestModel(bestModelWithiFeatures,"AdjRSq")
  return(bestModel)
}
)






x = ALMO_ETIP$new(dataset=iris[,-c(5)],
                    response="Sepal.Length"
)


x$findBestSubset()
x$forwardSelection()

x$backwardSelection()

```


## Problem 2
This problem focuses on the two methods of bootstrapping, Nonparametric and Parametric.  We are given a paired data set and asked to compute distributions for Fisher's z-transformation equation.  To start the code, we set up a function to compute the z-transformation.  Then we run the bootstrapp samples simultaneously in the data frame df_boot.  The Nonparametric approach resamples from the given data set, while the Parametric approach samples from the bivariate normal distribution with the sample mean, and covariance matrix given in the problem.  In the below graph the blue distribution shows the nonparametric bootstrapp sample centered around the red line which represents the observed z-transformation.  The orange distribution shows the parametric sample, centered at 0.  The parametric sample is centered here because we assumed no covariance in the data with the given covariance matrix.  If we were to use the observed covariance matrix from the data, the distributions would align more closley.

```{r}

pair = read.delim("pair.txt", sep = ",", header = F)

n = nrow(pair)

z = function(df) {
    r = cor(df)
    r = r[upper.tri(r)]
    0.5 * log((1 + r)/(1 - r))
}

xbar = apply(pair, 2, mean)

sigma = matrix(c(2, 0, 0, 4), 2)

phi = z(pair)

df = data.frame(indx = 1:2000)

df_boot = df %>% group_by(indx) %>%
                 mutate(Nonparametric = z(sample_n(pair, size = 200,replace = T)),
                        Parametric = z(mvrnorm(n = 200, xbar, sigma)))


df_b_long = df_boot %>% gather(indx)

ggplot(data = df_b_long, aes(x = value, fill = indx)) + geom_density(alpha = 0.75) + 
    scale_fill_brewer(palette = "Dark2") +
     geom_vline(aes(xintercept = phi), col = "red")+
     labs(title = "Bootstrapp Sample Distributions", x = "z",
            subtitle = "Red line represents observed z value",fill = "Type")
```


We are also asked to compute the confidence intervals for each method, plus the theoretical interval using fisher's formula.  The below code produces this table which gives these intervals.  We see the theoretical interval is much wider than the bootstrapp intervals.  This too is largley dependent on the number of samples in our bootstrapp distributions.  If we went with smaller bootstrapp sample sizes, we would see larger confidence intervals.

```{r}
data.frame( Nonparametric = quantile(df_boot$Nonparametric, c(0.025, 0.975)),
            Parametric = quantile(df_boot$Parametric, c(0.025, 0.975)),
            Theoretical=c(phi - 1.96/sqrt(n - 3), phi + 1.96/sqrt(n - 3))) %>%
  kable(caption='Confidence Intervals') %>%
  kable_styling(latex_options = "HOLD_position")
```

## Problem 3
\section{Summary of 6.2}
While subset selection methods allow to choose the best models that contain different number of predictors fitting the least squares, there is a technique that allows to use all the predictors with a constraint that reduces the coefficient estimates to the values close to zero. Such methods include ridge and lasso regression that provide significant variance reduction.
Ridge regression is the method of minimizing the following quantity:
$$
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_{j}^2 = RSS + \lambda \sum_{j=1}^p \beta_{j}^2
$$
where $\lambda$ is a tuning parameter that controls the effect of these two quantities on the coefficient estimates in the regression model and $\lambda \sum_{j=1}^p \beta_{j}^2$ is called a shrinkage penalty that reduces the coefficient estimates when $\lambda$ becomes really large, making the coefficient estimate approach zero, which corresponds to the null model – a model without any predictors. The intercept of the model, however, will not be affected because it represents the mean of the response providing that in the absence of the predictors there is no association with the response and the model will output the mean value of the response variable. For each value of $\lambda$ the new model will be produced, therefore, it is crucial to choose the good tuning parameter. When $\lambda$ is zero the penalty term will become zero and the ordinary least squares will be fitted.
Ridge regression must be applied with standardizing the predictors by the formula:
$$
\~x_{ij} = \frac{x_{ij}}{\sqrt{1/n \sum_{i=1}^n (x_{ij}-\bar{x_j})^2}}
$$
due to the fact that multiplying the predictors by a constant can significantly change the coefficient estimates. An example of such a dramatic change is the income variable that can be measured in dollars but might as well be measured in thousands of dollars such that the coefficient estimate will be increased by a factor of 1,000. Therefore, it is important to standardize the predictors - make the standard deviation equal to one, so that the model will not depend on the scale of predictors. The main advantage of the ridge regression over the ordinary least squares is that the increase in $\lambda$ up to a certain point decreases variance, increasing the bias slightly, achieving minimum MSE. In general, ridge regression will perform better than the least squares if number of predictors is greater than number of observations due to trade-off between decreased variance and increase in bias; if the relationship between the variables is linear then the ordinary least squares coefficients will have high variability. Another advantage of the ridge regression is computational feasibility – for a single value of $\lambda$ exactly one model will be produced, in comparison with the best subset selection that has $2^p$ models to consider.
While ridge regression is a good alternative to the best subset selection algorithms, it has one significant disadvantage – it will always include all the predictors in the model even though their coefficients are close to zero. Lasso regression provides the solution to this by minimizing slightly different quantity:
$$
\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p |\beta_{j}| = RSS + \lambda \sum_{j=1}^p |\beta_{j}|
$$
using $l_1$ norm instead of $l_2$ norm in the penalty term. Lasso regression shrinks the coefficient estimates towards zero, but in the contrast with ridge, they actually can become zero so that they are excluded from the model. In fact, lasso performs variable selection similarly to the best subset algorithms. Generally, it easier to interpret the coefficients produced by lasso rather than by ridge since the estimates can actually be zero.
It is possible to show that the problems solved by lasso and ridge are such that 
\begin{equation*}
\begin{aligned}
& \underset{\beta}{\text{minimize}}
& & \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \\
& \text{subject to}
& & \sum_{j=1}^p |\beta_j|\leq s
\end{aligned}
\end{equation*}
\begin{equation*}
\begin{aligned}
& \underset{\beta}{\text{minimize}}
& & \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \\
& \text{subject to}
& & \sum_{j=1}^p \beta_j^2\leq s
\end{aligned}
\end{equation*}
where $s$ is a constraint represented by the diamond $|\beta_1| + |\beta_2| \leq s$ in the first case and by the circle $\beta_{1}^2 + \beta_{2}^2 \leq s$ in the second case. If the constraint is not that restrictive, meaning $s$ is quite large, then ordinary least squares will satisfy the inequality providing least squares estimates. In other cases, if $s$ is small, then $\sum_{j=1}^p |\beta_j|$ or $\sum_{j=1}^p \beta_j^2$ have to be smaller in order not to violate the constraint.
Geometrically speaking, the lasso constraint represents a diamond and the RSS are ellipses spreading until one of them touches the border of the diamond – in many cases it will happen at the corner on the axis, making one of the coefficients zero. In the case of ridge regression, the constraint is a circle which RSS would often intersect not on the axis so that the coefficient estimates will be non-zero. In high-dimensional space lasso might make many coefficients zero simultaneously.
In general, lasso performs variable selection with coefficients that are easier to interpret, while ridge will decrease variance in exchange to a slight increase in bias.
Consider a special case when number of predictors is the same as the number of observations, and the matrix of predictors consists of 1’s on the diagonal and 0’s off that. Then, the problem of minimizing two quantities in the ridge regression simplifies to: 
$$
\sum_{j=1}^p (y_i  - \beta_{j})^2 + \lambda \sum_{j=1}^p \beta_{j}^2
$$
And in lasso regression to 
$$
\sum_{j=1}^p (y_i  - \beta_{j})^2 + \lambda \sum_{j=1}^p |\beta_{j}|
$$
so that the coefficient estimates take the form $\hat{\beta_{j}^R} = y_i/(1+\lambda)$ in the ridge and,
$$
\hat{\beta}_{j}^L = \left\{
        \begin{array}{ll}
            y_i - \lambda/2 & \quad y_j > \lambda/2 \\
            y_i + \lambda/2 & \quad y_j < -\lambda/2 \\
            0 & \quad |y_j| \leq \lambda/2
        \end{array}
    \right.
$$
lasso respectively. It is easy to see that all the coefficients in the ridge setting are shrunk by the same proportion, while in lasso the coefficients are compared with the absolute value of $\lambda/2$ and shrunk by more or less the same amount, or entirely to zero if the coefficient is less than or equal to $\lambda/2$. This type of shrinkage is called soft thresholding.
Consider the Bayesian perspective of ridge and lasso regression – posterior distribution is proportional to the product of prior and likelihood of the data up to a proportionality constant:
$$
p(\beta|X,Y) \propto f(Y|X, \beta)p(\beta|X) = f(Y|X,\beta)p(\beta)
$$

Using the linear model with errors that follow normal distribution, assume that prior distribution of coefficient estimates is some function of $g$: $p(\beta) = {\prod_{j=1}^p g(\beta_j)}$. Then the ridge regression solution is the posterior mode for $\beta$, if $g$ is Gaussian family with the mean zero and standard deviation being a function of $\lambda$. In case when $g$ is double-exponential with the mean zero and scale parameter is a function of $\lambda$- lasso solution if the posterior mode of $\beta$.
Selecting the tuning parameter lambda is crucial for ridge and lasso regression. Cross validation helps determine the value that makes its error the smallest. First, the cross-validated error is computed for the grid of $\lambda$ values. The final model is refitted with the selected value and all the observations.

## Problem 4

### Question 5
The problem asks us to build a logistic regression model using the default data set, first with income and balance as the predictors, then with student included.  We are to use a validation set approach, computing the validation set error, and repeat this three times for each model.  We first set up a specific function to split the data into training and testing sets, train the model on the training set, then use the model to make the predictions on the test set.  Then we calculate the missclassification rate, which is the fraction of observations in the test set that were missclassified.  To extend this a little further we included the precision and recall rates.  These are specific to the type I and II errors of the model predictions.  Given the low rate of defaults in the data set, we might want to pay closer attention to the recall rate.  We do not see much change between the three iterations.  While the overall missclassification rate is fairly low, the recall is very low around 30%.  This represents how many of the defaults we actually predicted correctly.  Adding student to the model, there is not much change in the missclassification rate or the recall rate.

```{r}

default = read.csv("Default.csv")

dflt_n = nrow(default)
default$default = recode(default$default, No = 0, Yes = 1)


dflt_log = function(seed, formula) {
    set.seed(seed)
    dflt_trn_idx = sample(1:dflt_n, dflt_n * 0.8)
    dflt_trn = default[dflt_trn_idx, ]
    dflt_tst = default[-dflt_trn_idx, ]
    dflt_fit = glm(formula, data = dflt_trn, family = binomial())
    dflt_tst = dflt_tst %>%
            mutate(prd = ifelse(
                            predict(dflt_fit, dflt_tst, type = "response") > 0.5, 1, 0),
                   correct = prd == default)
    dflt_errs = dflt_tst %>% mutate(l = prd == default) %>%
                group_by(default) %>% 
                summarise(rate = mean(l))
    data.frame(Accuracy = mean(dflt_tst$correct),
            Missclassification = 1 - mean(dflt_tst$correct),
            Precision = as.double(dflt_errs[1, 2]), 
            Recall = as.double(dflt_errs[2, 2]))
}


dflt_tries = data.frame(seed = sample(1:1000, 3))

apply(dflt_tries, 1, function(x) dflt_log(x, formula = default ~ income + balance)) 

dflt_tries2 = data.frame(seed = sample(1:1000, 3))

apply(dflt_tries2, 1, function(x) dflt_log(x, formula = default ~ income + balance + student)) 
```

### Question 6

### Question 9
```{r}
library(MASS)
attach(Boston)
muh <- mean(medv)
muh
seh <- sd(medv) / sqrt(dim(Boston)[1])
seh
library(boot)
boot.f <- function(data, index) {
  mu <- mean(data[index])
  return (mu)
}
boot(medv, boot.fn, 1000)
t.test(medv)
CI.mu.hat <- c(22.53 - 2 * 0.4119, 22.53 + 2 * 0.4119)
CI.mu.hat
medh <- median(medv)
medh
boot.f1 <- function(data, index) {
  mu <- median(data[index])
  return (mu)
}
boot(medv, boot.f1, 1000)
p.hat <- quantile(medv, c(0.1))
p.hat


```



## Appendix
